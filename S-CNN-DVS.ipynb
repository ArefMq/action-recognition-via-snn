{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_spikes_in_time, print_and_plot_accuracy_metrics, plot_metrics\n",
    "from scnn import SNN\n",
    "from scnn.optim import RAdam\n",
    "\n",
    "from data.data_augmentor import data_augment, batchify\n",
    "from tools.time_expector import TimeExpector\n",
    "time_expector = TimeExpector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "nb_epochs = 6\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float\n",
    "\n",
    "test_run = True\n",
    "if test_run:\n",
    "    print('[WARNING] : This is test run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME\n",
    "my_laptop = True\n",
    "if my_laptop:\n",
    "    CACHE_FOLDER_PATH = \"/Users/aref/dvs-dataset/Cached\"\n",
    "    DATASET_FOLDER_PATH = \"/Users/aref/dvs-dataset/DvsGesture\"\n",
    "else:\n",
    "    CACHE_FOLDER_PATH = \"/home/aref/dataset/dvs-dataset\"\n",
    "    DATASET_FOLDER_PATH = \"/home/aref/dataset/dvs-dataset\"\n",
    "\n",
    "    \n",
    "def load_data(trail):\n",
    "    if test_run:\n",
    "        trail = 'acc_test' # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Remove this >>>>>>>>>>>>>>>>\n",
    "    \n",
    "    if trail.startswith('acc'):\n",
    "        max_augmentation = 1\n",
    "        augmentation = False\n",
    "    else:\n",
    "        max_augmentation = 3 if trail == 'train' else 1\n",
    "        augmentation = True\n",
    "    \n",
    "    trail = trail.replace('acc_', '')\n",
    "    return batchify(\n",
    "        trail,\n",
    "        DATASET_FOLDER_PATH,\n",
    "        CACHE_FOLDER_PATH,\n",
    "        condition_limit=['natural'],\n",
    "        batch_size=batch_size,\n",
    "        augmentation=augmentation,\n",
    "        max_augmentation=max_augmentation,\n",
    "        frame=60\n",
    "    )\n",
    "\n",
    "# calculate train dataset size\n",
    "dataset_size = [0., 0.]\n",
    "for x_batch, y_batch in load_data('train'):\n",
    "    dataset_size[0] += 1.\n",
    "    if dataset_size[0] % 64 == 1:\n",
    "        print('\\rpre-processing dataset: %d' % dataset_size[0], end='')\n",
    "print('\\rpre-processing dataset: %d' % dataset_size[0])\n",
    "for x_batch, y_batch in load_data('test'):\n",
    "    dataset_size[1] += 1.\n",
    "    if dataset_size[1] % 64 == 1:\n",
    "        print('\\rpre-processing dataset: %d' % dataset_size[1], end='')\n",
    "print('\\rpre-processing dataset: %d' % dataset_size[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_one_batch(network):\n",
    "    for X_batch, _ in load_data('train'):\n",
    "        break\n",
    "\n",
    "    network.predict(X_batch)\n",
    "\n",
    "    for i,l in enumerate(network.layers):\n",
    "        if 'spk_rec_hist' in l.__dict__:\n",
    "            print(\"Layer {}: average number of spikes={:.4f}\".format(i, l.spk_rec_hist.mean()))\n",
    "            if l.HAS_PARAM:\n",
    "                plot_spikes_in_time(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scnn.default_configs import *\n",
    "\n",
    "\n",
    "class MyConv3d(torch.nn.Module):\n",
    "    IS_CONV = True\n",
    "    IS_SPIKING = True\n",
    "    HAS_PARAM = True\n",
    "\n",
    "    def __init__(self, input_shape, input_channels, output_shape=None,\n",
    "                 output_channels=1, kernel_size=3, dilation=1,\n",
    "                 spike_fn=None, w_init_mean=W_INIT_MEAN, w_init_std=W_INIT_STD, recurrent=False,\n",
    "                 lateral_connections=True,\n",
    "                 eps=EPSILON, stride=(1, 1, 1), flatten_output=False):\n",
    "\n",
    "        super(MyConv3d, self).__init__()\n",
    "\n",
    "        self.kernel_size = np.array(kernel_size)\n",
    "        self.dilation = np.array(dilation)\n",
    "        self.stride = np.array(stride)\n",
    "        self.input_channels = input_channels\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        self.output_channels = output_channels\n",
    "        self.output_shape = output_shape if output_shape is not None else input_shape\n",
    "        self.spike_fn = spike_fn\n",
    "        self.recurrent = recurrent\n",
    "        self.lateral_connections = lateral_connections\n",
    "        self.eps = eps\n",
    "\n",
    "        self.flatten_output = flatten_output\n",
    "\n",
    "        self.w_init_mean = w_init_mean\n",
    "        self.w_init_std = w_init_std\n",
    "\n",
    "        self.w = torch.nn.Parameter(torch.empty((output_channels, input_channels, *kernel_size)), requires_grad=True)\n",
    "        if recurrent:\n",
    "            self.v = torch.nn.Parameter(torch.empty((output_channels, output_channels)), requires_grad=True)\n",
    "        self.beta = torch.nn.Parameter(torch.empty(1), requires_grad=True)\n",
    "        self.b = torch.nn.Parameter(torch.empty(output_channels), requires_grad=True)\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.clamp()\n",
    "\n",
    "        self.spk_rec_hist = None\n",
    "        self.mem_rec_hist = None\n",
    "        self.training = True\n",
    "\n",
    "        # TODO : check this\n",
    "        tau_mem = 10e-3\n",
    "        tau_syn = 5e-3\n",
    "        time_step = 1e-3\n",
    "        self._alpha = float(np.exp(-time_step / tau_syn))\n",
    "        self._beta = float(np.exp(-time_step / tau_mem))\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        res = [\n",
    "            {'params': self.w},  #, 'lr': lr, \"weight_decay\": DEFAULT_WEIGHT_DECAY}\n",
    "            {'params': self.b},\n",
    "            {'params': self.beta},\n",
    "        ]\n",
    "\n",
    "        if self.recurrent:\n",
    "            res.append({'params': self.v})\n",
    "        return res\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        nb_steps = x.shape[2]\n",
    "        \n",
    "        stride = tuple(self.stride)\n",
    "        padding = tuple(np.ceil(((self.kernel_size - 1) * self.dilation) / 2).astype(int))\n",
    "        conv_x = torch.nn.functional.conv3d(x, self.w, padding=padding,\n",
    "                                            dilation=tuple(self.dilation),\n",
    "                                            stride=stride)\n",
    "        conv_x = conv_x[:, :, :, :self.output_shape[0], :self.output_shape[1]]\n",
    "\n",
    "        mem = torch.zeros((batch_size, self.output_channels, *self.output_shape), dtype=x.dtype, device=x.device)\n",
    "        syn = torch.zeros((batch_size, self.output_channels, *self.output_shape), device=x.device, dtype=x.dtype)  # FIXME\n",
    "        spk = torch.zeros((batch_size, self.output_channels, *self.output_shape), dtype=x.dtype, device=x.device)\n",
    "\n",
    "        spk_rec = torch.zeros((batch_size, self.output_channels, nb_steps, *self.output_shape), dtype=x.dtype, device=x.device)\n",
    "        mem_rec = torch.zeros((batch_size, self.output_channels, nb_steps, *self.output_shape), dtype=x.dtype, device=x.device)\n",
    "\n",
    "        if self.lateral_connections:\n",
    "            d = torch.einsum(\"abcde, fbcde -> af\", self.w, self.w)\n",
    "        b = self.b.unsqueeze(1).unsqueeze(1).repeat((1, *self.output_shape))\n",
    "\n",
    "        norm = (self.w ** 2).sum((1, 2, 3, 4))\n",
    "\n",
    "        for t in range(nb_steps):\n",
    "            # spike term\n",
    "            # mthr = torch.einsum(\"abcd,b->abcd\", mem, 1. / (norm + self.eps)) - b\n",
    "            # spk = self.spike_fn(mthr)\n",
    "\n",
    "            if self.lateral_connections:\n",
    "                rst = torch.einsum(\"abcd,be ->aecd\", spk, d)\n",
    "            else:\n",
    "                rst = torch.einsum(\"abcd,b,b->abcd\", spk, self.b, norm)\n",
    "\n",
    "            input_ = conv_x[:, :, t, :, :]\n",
    "            if self.recurrent:\n",
    "                input_ = input_ + torch.einsum(\"abcd,be->aecd\", spk, self.v)\n",
    "\n",
    "            # TODO : check to see if this is actually works\n",
    "            new_syn = self._alpha * syn + input_\n",
    "            new_mem = self._beta * mem + syn - rst\n",
    "            mem = new_mem\n",
    "            syn = new_syn\n",
    "            # mem = (mem - rst) * self.beta + input_ * (1. - self.beta)\n",
    "\n",
    "            mthr = torch.einsum(\"abcd,b->abcd\", mem, 1. / (norm + self.eps)) - b\n",
    "            spk = self.spike_fn(mthr)\n",
    "\n",
    "            spk_rec[:, :, t, :, :] = spk\n",
    "            mem_rec[:, :, t, :, :] = mem\n",
    "\n",
    "        self.spk_rec_hist = spk_rec.detach().cpu().numpy()\n",
    "        self.mem_rec_hist = mem_rec.detach().cpu().numpy()\n",
    "\n",
    "        if self.flatten_output:\n",
    "            output = torch.transpose(spk_rec, 1, 2).contiguous()\n",
    "            output = output.view(batch_size, nb_steps, self.output_channels * np.prod(self.output_shape))\n",
    "        else:\n",
    "            output = spk_rec\n",
    "\n",
    "        return output\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.normal_(self.w, mean=self.w_init_mean,\n",
    "                              std=self.w_init_std * np.sqrt(1. / (self.input_channels * np.prod(self.kernel_size))))\n",
    "        if self.recurrent:\n",
    "            torch.nn.init.normal_(self.v, mean=self.w_init_mean,\n",
    "                                  std=self.w_init_std * np.sqrt(1. / self.output_channels))\n",
    "        torch.nn.init.normal_(self.beta, mean=0.7, std=0.01)\n",
    "        torch.nn.init.normal_(self.b, mean=1., std=0.01)\n",
    "\n",
    "    def clamp(self):\n",
    "        self.beta.data.clamp_(0., 1.)\n",
    "        self.b.data.clamp_(min=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SNN(device=device, dtype=dtype)\n",
    "network.time_expector = time_expector\n",
    "\n",
    "\n",
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "time_step = 1e-3\n",
    "beta = float(np.exp(-time_step / tau_mem))\n",
    "weight_scale = 7*(1.0 - beta)\n",
    "\n",
    "# network.add_dense( #layer(LegacyDense,\n",
    "#     input_shape=4096,\n",
    "#     output_shape=128,              \n",
    "#     w_init_mean=0.0,\n",
    "#     w_init_std=weight_scale\n",
    "# )\n",
    "\n",
    "# network.add_layer(NewSpiker,\n",
    "#     input_shape=4096,\n",
    "#     output_shape=128,\n",
    "                  \n",
    "#     w_init_mean=0.0,\n",
    "#     w_init_std=weight_scale\n",
    "# )\n",
    "\n",
    "network.add_layer(MyConv3d, #conv3d(\n",
    "    input_shape=(64,64),\n",
    "    \n",
    "    output_channels=16,\n",
    "    kernel_size=(1,3,3),\n",
    "    dilation=(1,1,1),\n",
    "    lateral_connections=False,\n",
    "    \n",
    "    w_init_mean=1.0,\n",
    "    w_init_std=0.01\n",
    ")\n",
    "\n",
    "network.add_pool2d(kernel_size=(4,4))\n",
    "\n",
    "\n",
    "# network.add_dense(\n",
    "#     output_shape=128,\n",
    "#     w_init_mean=0.0,\n",
    "#     w_init_std=weight_scale\n",
    "# #     lateral_connections=True\n",
    "# )\n",
    "\n",
    "# network.add_layer(SpikingDenseLayer,\n",
    "#     output_shape=256\n",
    "# )\n",
    "\n",
    "# network.add_layer(SpikingDenseLayer,\n",
    "#     output_shape=128,\n",
    "#     w_init_mean=.19\n",
    "# )\n",
    "\n",
    "network.add_readout(output_shape=12,\n",
    "                    time_reduction=\"max\" # mean or max\n",
    ")\n",
    "\n",
    "network.compile()\n",
    "network = network.to(network.device, network.dtype) # FIXME: this is a bug, fix it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_one_batch(network)\n",
    "print_and_plot_accuracy_metrics(network, load_data('acc_train'), load_data('acc_test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('results.log', 'w') as f:\n",
    "    # opt = RAdam(network.get_trainable_parameters())\n",
    "    opt = torch.optim.SGD(network.get_trainable_parameters(), lr=1e-3, momentum=0.9)\n",
    "    res_metrics = network.fit(\n",
    "        load_data, \n",
    "        epochs=nb_epochs,\n",
    "        optimizer=opt, \n",
    "        dataset_size=dataset_size, \n",
    "        result_file=f\n",
    "    )\n",
    "    plot_metrics(res_metrics)\n",
    "\n",
    "network.save('save_network.net')\n",
    "# network.load('save_network.net')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_batch(network)\n",
    "print_and_plot_accuracy_metrics(network, load_data('acc_train'), load_data('acc_test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(batch_size):\n",
    "    for i in range(12):\n",
    "        plt.plot(network.layers[-1].mem_rec_hist[b,:,i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# network.predict(X_batch)\n",
    "\n",
    "# # Plotting spike trains or membrane potential\n",
    "# for i,l in enumerate(network.layers):\n",
    "#     if not l.HAS_PARAM or 'spk_rec_hist' not in l.__dict__:\n",
    "#         continue\n",
    "        \n",
    "#     if isinstance(l, SpikingDenseLayer):\n",
    "#         print(\"Layer {}: average number of spikes={:.4f}\".format(i,l.spk_rec_hist.mean()))\n",
    "#         spk_rec = l.spk_rec_hist\n",
    "#         plot_spk_rec(spk_rec, idx=batch_idx)\n",
    "#     elif isinstance(l, SpikingConv2DLayer):\n",
    "#         print(\"Layer {}: average number of spikes={:.4f}\".format(i,l.spk_rec_hist.mean()))\n",
    "#         spk_rec = l.spk_rec_hist\n",
    "#         plot_spk_rec(spk_rec.sum(1), idx=batch_idx)\n",
    "#     else:\n",
    "#         mem_rec = l.mem_rec_hist\n",
    "#         plot_mem_rec(mem_rec, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
