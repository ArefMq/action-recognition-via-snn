{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "SAVE_PLOTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_spikes_in_time, print_and_plot_accuracy_metrics, plot_metrics\n",
    "from scnn import SNN\n",
    "from scnn.optim import RAdam\n",
    "\n",
    "from data.data_augmentor import data_augment, batchify\n",
    "from tools.time_expector import TimeExpector\n",
    "from tools.notify import notify\n",
    "time_expector = TimeExpector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "nb_epochs = 2\n",
    "nb_frame = 40\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# test_run = True\n",
    "# if test_run:\n",
    "#     print('[WARNING] : This is test run.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# FIXME\n",
    "my_laptop = False\n",
    "if my_laptop:\n",
    "    CACHE_FOLDER_PATH = \"/Users/aref/dvs-dataset/Cached\"\n",
    "    DATASET_FOLDER_PATH = \"/Users/aref/dvs-dataset/DvsGesture\"\n",
    "else:\n",
    "    CACHE_FOLDER_PATH = \"/home/aref/dataset/dvs-dataset\"\n",
    "    DATASET_FOLDER_PATH = \"/home/aref/dataset/dvs-dataset\"\n",
    "\n",
    "    \n",
    "# def load_data(trail):\n",
    "# #     if test_run:\n",
    "# #         trail = 'acc_test' # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Remove this >>>>>>>>>>>>>>>>\n",
    "    \n",
    "#     if trail.startswith('acc'):\n",
    "#         max_augmentation = 1\n",
    "#         augmentation = False\n",
    "#     else:\n",
    "#         max_augmentation = 2 if trail == 'train' else 1\n",
    "#         augmentation = True\n",
    "    \n",
    "#     trail = trail.replace('acc_', '')\n",
    "#     return batchify(\n",
    "#         trail,\n",
    "#         DATASET_FOLDER_PATH,\n",
    "#         CACHE_FOLDER_PATH,\n",
    "#         condition_limit=['natural'],\n",
    "#         batch_size=batch_size,\n",
    "#         augmentation=augmentation,\n",
    "#         max_augmentation=max_augmentation,\n",
    "#         frame=nb_frame\n",
    "#     )\n",
    "\n",
    "original_size = 64\n",
    "for __xb, __yb in batchify(\n",
    "    'train',\n",
    "    DATASET_FOLDER_PATH,\n",
    "    CACHE_FOLDER_PATH,\n",
    "    condition_limit=['natural'],\n",
    "    batch_size=original_size,\n",
    "    augmentation=False,\n",
    "    max_augmentation=1,\n",
    "    frame=nb_frame\n",
    "):\n",
    "    break\n",
    "\n",
    "_hist = {i:0 for i in range(12)}\n",
    "for i in __yb:\n",
    "    _hist[i] += 1\n",
    "max_value = max(_hist.values())\n",
    "\n",
    "aug_xb = []\n",
    "aug_yb = []\n",
    "for i in range(12):\n",
    "    idx = np.where(__yb == i)[0][0]\n",
    "    to_add = max_value - _hist[i]\n",
    "    for _ in range(to_add):\n",
    "        aug_yb.append(i)\n",
    "        aug_xb.append(__xb[idx, :, :, :])\n",
    "\n",
    "print('pre augmentation size:')\n",
    "print(__xb.shape)\n",
    "print(__yb.shape)\n",
    "__yb = np.concatenate([__yb, np.array(aug_yb)])\n",
    "__xb = np.concatenate([__xb, np.array(aug_xb)])\n",
    "print('\\npost augmentation size:')\n",
    "print(__xb.shape)\n",
    "print(__yb.shape)\n",
    "\n",
    "\n",
    "def load_data(trail):\n",
    "    begin = 0\n",
    "    end = begin + batch_size\n",
    "    while end <= __yb.shape[0]:\n",
    "        yield __xb[begin:end, :, :, :], __yb[begin:end]\n",
    "        begin = end\n",
    "        end = begin + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate train dataset size\n",
    "train_hist = {i:0 for i in range(12)}\n",
    "test_hist = {i:0 for i in range(12)}\n",
    "dataset_size = [0., 0.]\n",
    "for x_batch, y_batch in load_data('train'):\n",
    "    for i in y_batch:\n",
    "        train_hist[i] += 1\n",
    "    dataset_size[0] += 1.\n",
    "    if dataset_size[0] % 64 == 1:\n",
    "        print('\\rpre-processing dataset: %d' % dataset_size[0], end='')\n",
    "print('\\rpre-processing dataset: %d' % dataset_size[0])\n",
    "\n",
    "for x_batch, y_batch in load_data('test'):\n",
    "    for i in y_batch:\n",
    "        test_hist[i] += 1\n",
    "    dataset_size[1] += 1.\n",
    "    if dataset_size[1] % 64 == 1:\n",
    "        print('\\rpre-processing dataset: %d' % dataset_size[1], end='')\n",
    "print('\\rpre-processing dataset: %d' % dataset_size[1])\n",
    "\n",
    "plt.bar(list(train_hist.keys()), list(train_hist.values()))\n",
    "plt.title('Train Dataset Distribution')\n",
    "plt.show()\n",
    "plt.bar(list(test_hist.keys()), list(test_hist.values()))\n",
    "plt.title('Test Dataset Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_one_batch(network):\n",
    "    for X_batch, _ in load_data('train'):\n",
    "        break\n",
    "\n",
    "    network.predict(X_batch)\n",
    "\n",
    "    for i,l in enumerate(network.layers):\n",
    "        if 'spk_rec_hist' in l.__dict__:\n",
    "            print(\"Layer {}: average number of spikes={:.4f}\".format(i, l.spk_rec_hist.mean()))\n",
    "        plot_spikes_in_time(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SNN(device=device, dtype=dtype)\n",
    "network.time_expector = time_expector\n",
    "network.notifier = notify\n",
    "\n",
    "\n",
    "# tau_mem = 10e-3\n",
    "# tau_syn = 5e-3\n",
    "# time_step = 1e-3\n",
    "# beta = float(np.exp(-time_step / tau_mem))\n",
    "# weight_scale = 7*(1.0 - beta) # =.6\n",
    "\n",
    "# network.add_dense(\n",
    "#     input_shape=(64,64),\n",
    "#     output_shape=128,              \n",
    "#     w_init_mean=0.0,\n",
    "#     w_init_std=weight_scale\n",
    "# )\n",
    "\n",
    "network.add_conv3d(\n",
    "    input_shape=(64,64),\n",
    "    \n",
    "    output_channels=4,#32,\n",
    "    kernel_size=(1,3,3),\n",
    "    dilation=(1,1,1),\n",
    "#     lateral_connections=True,\n",
    "#     recurrent=True,\n",
    "    \n",
    "    w_init_mean=0.00,\n",
    "    w_init_std=0.005\n",
    ")\n",
    "network.add_pool2d(kernel_size=(16,16))\n",
    "\n",
    "network.add_dense(\n",
    "    output_shape=128,\n",
    "    w_init_mean=0.0,\n",
    "    w_init_std=0.6\n",
    "#     lateral_connections=True\n",
    ")\n",
    "\n",
    "network.add_readout(\n",
    "    output_shape=12,\n",
    "    time_reduction=\"max\",\n",
    "\n",
    "    w_init_mean=0.0,\n",
    "    w_init_std=0.5\n",
    ")\n",
    "\n",
    "network.compile()\n",
    "network = network.to(network.device, network.dtype) # FIXME: this is a bug, fix it!\n",
    "\n",
    "print('Network Summery:', network.serialize())\n",
    "plot_one_batch(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('results.log', 'w+') as f:\n",
    "    lr=0.01\n",
    "    opt = RAdam(network.get_trainable_parameters(lr))\n",
    "#     opt = torch.optim.SGD(network.get_trainable_parameters(lr), lr=lr, momentum=0.9)\n",
    "    res_metrics = network.fit(\n",
    "        load_data, \n",
    "        epochs=nb_epochs,\n",
    "        optimizer=opt, \n",
    "        dataset_size=dataset_size, \n",
    "        result_file=f,\n",
    "        save_checkpoints=False\n",
    "    )\n",
    "    plot_metrics(res_metrics, save_plot_path='./metrics_' if SAVE_PLOTS else None)\n",
    "\n",
    "# network.save('save_network.net')\n",
    "# network.load('save_network.net')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_one_batch(network)\n",
    "print_and_plot_accuracy_metrics(\n",
    "    network, \n",
    "    load_data('acc_train'), \n",
    "    load_data('acc_test'), \n",
    "    save_plot_path='./truth_' if SAVE_PLOTS else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# network.predict(X_batch)\n",
    "\n",
    "# # Plotting spike trains or membrane potential\n",
    "# for i,l in enumerate(network.layers):\n",
    "#     if not l.HAS_PARAM or 'spk_rec_hist' not in l.__dict__:\n",
    "#         continue\n",
    "        \n",
    "#     if isinstance(l, SpikingDenseLayer):\n",
    "#         print(\"Layer {}: average number of spikes={:.4f}\".format(i,l.spk_rec_hist.mean()))\n",
    "#         spk_rec = l.spk_rec_hist\n",
    "#         plot_spk_rec(spk_rec, idx=batch_idx)\n",
    "#     elif isinstance(l, SpikingConv2DLayer):\n",
    "#         print(\"Layer {}: average number of spikes={:.4f}\".format(i,l.spk_rec_hist.mean()))\n",
    "#         spk_rec = l.spk_rec_hist\n",
    "#         plot_spk_rec(spk_rec.sum(1), idx=batch_idx)\n",
    "#     else:\n",
    "#         mem_rec = l.mem_rec_hist\n",
    "#         plot_mem_rec(mem_rec, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on larger dataset\n",
    "original_size = 700\n",
    "for __xb, __yb in batchify(\n",
    "    'test',\n",
    "    DATASET_FOLDER_PATH,\n",
    "    CACHE_FOLDER_PATH,\n",
    "    condition_limit=['natural'],\n",
    "    batch_size=original_size,\n",
    "    augmentation=False,\n",
    "    max_augmentation=1,\n",
    "    frame=nb_frame\n",
    "):\n",
    "    break\n",
    "\n",
    "_hist = {i:0 for i in range(12)}\n",
    "for i in __yb:\n",
    "    _hist[i] += 1\n",
    "max_value = max(_hist.values())\n",
    "\n",
    "aug_xb = []\n",
    "aug_yb = []\n",
    "for i in range(12):\n",
    "    idx = np.where(__yb == i)[0][0]\n",
    "    to_add = max_value - _hist[i]\n",
    "    for _ in range(to_add):\n",
    "        aug_yb.append(i)\n",
    "        aug_xb.append(__xb[idx, :, :, :])\n",
    "\n",
    "print('pre augmentation size:')\n",
    "print(__xb.shape)\n",
    "print(__yb.shape)\n",
    "__yb = np.concatenate([__yb, np.array(aug_yb)])\n",
    "__xb = np.concatenate([__xb, np.array(aug_xb)])\n",
    "print('\\npost augmentation size:')\n",
    "print(__xb.shape)\n",
    "print(__yb.shape)\n",
    "\n",
    "\n",
    "def load_data(trail):\n",
    "    begin = 0\n",
    "    end = begin + batch_size\n",
    "    while end <= __yb.shape[0]:\n",
    "        yield __xb[begin:end, :, :, :], __yb[begin:end]\n",
    "        begin = end\n",
    "        end = begin + batch_size\n",
    "\n",
    "\n",
    "# calculate train dataset size\n",
    "dataset_size = [0., 0.]\n",
    "for x_batch, y_batch in load_data('train'):\n",
    "    dataset_size[0] += 1.\n",
    "    if dataset_size[0] % 64 == 1:\n",
    "        print('\\rpre-processing dataset: %d' % dataset_size[0], end='')\n",
    "print('\\rpre-processing dataset: %d' % dataset_size[0])\n",
    "\n",
    "for x_batch, y_batch in load_data('test'):\n",
    "    dataset_size[1] += 1.\n",
    "    if dataset_size[1] % 64 == 1:\n",
    "        print('\\rpre-processing dataset: %d' % dataset_size[1], end='')\n",
    "print('\\rpre-processing dataset: %d' % dataset_size[1])\n",
    "\n",
    "\n",
    "with open('results.log', 'w+') as f:\n",
    "    lr=0.001\n",
    "    opt = RAdam(network.get_trainable_parameters(lr))\n",
    "#     opt = torch.optim.SGD(network.get_trainable_parameters(lr), lr=lr, momentum=0.9)\n",
    "    res_metrics = network.fit(\n",
    "        load_data, \n",
    "        epochs=10,\n",
    "        optimizer=opt, \n",
    "        dataset_size=dataset_size, \n",
    "        result_file=f,\n",
    "        save_checkpoints=False\n",
    "    )\n",
    "    plot_metrics(res_metrics, save_plot_path='./metrics_B_' if SAVE_PLOTS else None)\n",
    "\n",
    "\n",
    "plot_one_batch(network)\n",
    "print_and_plot_accuracy_metrics(\n",
    "    network, \n",
    "    load_data('acc_train'), \n",
    "    load_data('acc_test'), \n",
    "    save_plot_path='./accuracy_B_' if SAVE_PLOTS else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run on full dataset\n",
    "\n",
    "def load_data(trail):\n",
    "    if trail.startswith('acc'):\n",
    "        max_augmentation = 1\n",
    "        augmentation = False\n",
    "    else:\n",
    "        max_augmentation = 2 if trail == 'train' else 1\n",
    "        augmentation = True\n",
    "    \n",
    "    trail = trail.replace('acc_', '')\n",
    "    return batchify(\n",
    "        trail,\n",
    "        DATASET_FOLDER_PATH,\n",
    "        CACHE_FOLDER_PATH,\n",
    "        condition_limit=['natural'],\n",
    "        batch_size=batch_size,\n",
    "        augmentation=augmentation,\n",
    "        max_augmentation=max_augmentation,\n",
    "        frame=nb_frame\n",
    "    )\n",
    "\n",
    "dataset_size = [0., 0.]\n",
    "for x_batch, y_batch in load_data('train'):\n",
    "    dataset_size[0] += 1.\n",
    "    if dataset_size[0] % 64 == 1:\n",
    "        print('\\rpre-processing dataset: %d' % dataset_size[0], end='')\n",
    "print('\\rpre-processing dataset: %d' % dataset_size[0])\n",
    "\n",
    "for x_batch, y_batch in load_data('test'):\n",
    "    dataset_size[1] += 1.\n",
    "    if dataset_size[1] % 64 == 1:\n",
    "        print('\\rpre-processing dataset: %d' % dataset_size[1], end='')\n",
    "print('\\rpre-processing dataset: %d' % dataset_size[1])\n",
    "\n",
    "\n",
    "with open('results.log', 'w+') as f:\n",
    "    lr=0.0001\n",
    "    opt = RAdam(network.get_trainable_parameters(lr))\n",
    "#     opt = torch.optim.SGD(network.get_trainable_parameters(lr), lr=lr, momentum=0.9)\n",
    "    res_metrics = network.fit(\n",
    "        load_data, \n",
    "        epochs=3,\n",
    "        optimizer=opt, \n",
    "        dataset_size=dataset_size, \n",
    "        result_file=f,\n",
    "        save_checkpoints=False\n",
    "    )\n",
    "    plot_metrics(res_metrics, save_plot_path='./metrics_C_' if SAVE_PLOTS else None)\n",
    "\n",
    "network.save('save_network.net')\n",
    "\n",
    "\n",
    "plot_one_batch(network)\n",
    "print_and_plot_accuracy_metrics(\n",
    "    network, \n",
    "    load_data('acc_train'), \n",
    "    load_data('acc_test'), \n",
    "    save_plot_path='./accuracy_C_' if SAVE_PLOTS else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
